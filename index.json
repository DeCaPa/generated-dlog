[{
    "title": "Shift Left? Shift Everywhere!",
    "date": "",
    "description": "What direction do we go after we’ve shifted left? How about everywhere else!",
    "body": "Shift Left? Shift Everywhere! The notion of shifting left has been a best practice by application security practitioners, devOps engineers, and application security tool vendors for a long time. Shift Left is a term used to reflect the practice of identifying weaknesses and vulnerabilities as quickly as possible, while minimizing development friction. The advantage to identifying issues early on, results in better security, agility, and cost efficiencies.\nLeft Until You Cannot Shift Left is accomplished by utilizing secure code analysis tools, trained secure developers, and automation to provide feedback at the speed of DevOps. Moving farther left is intended to provide results without the need to exit source code, wait on check-in, or automated scanning. It is accomplished by utilizing Integration Development Environment (IDE) extensions to identify and highlight the line where the flaw is found. Some plugins have a capability to analyze code while it is being written. Now that is as far left as one can go!\nEverywhere Else What is meant by “shifting everywhere”? In a previous post, I provided a brief list of decent open source application security tools. I categorized them by where they fit within the Secure Development Lifecycle (SDL). Just as a screwdriver and a hammer serve different purposes in the construction of a house, security tools have different uses within the SDL. Implementing security tools across all SDL phases is like placing security everywhere. Hence, “Shift Everywhere”!\nSecurity Pipelines How do we do this? Security pipelines combined with automation. Shifting everywhere has several implications, and there is no single solution that suits all organizations.\n“Whoa, whoa, whoa”, I imagine more than a few readers thinking, “If it isn’t in my Continuous Integration, Continuous Deployment (CI/CD) pipeline then it isn’t DevSecOps”. In order to shift everywhere, more security tools will be introduced across the lifecycle as maturity increases. As a result, CI/CD velocity could be impacted and add unwanted friction. Security pipelines need to extend beyond code check-in and deployment. It is a noble intention to try to run everything inline within a single pipeline, but reality may get in the way.\nLong scan times can leave a developer wondering if the check-in passes a policy or breaks a build. QA security tests, DAST, and other automated tests take time. A security pipeline running asynchronously might be a better fit.\nThe security pipeline will extend across all SDL phases. It is more than CI where code devdevelopment and static scans take place. How you architect is up to you team\u0026rsquo;s and company\u0026rsquo;s particular requirements.\nIn summary A holistic and balanced approach to Application Security inclusive of all phases of the SDL is the cornerstone of Shift Everywhere. In the age of Agile and continuous release methodologies, secure practices are injected into each iterative phase based on the organization’s or the product’s strategic risk acceptance \u0026amp; avoidance criteria.\nShifting left is a great approach for application security and product teams. Once there, shifting everywhere else is the next step on the path towards greater application security maturity. Development teams continuously add new features to product releases with each iteration. It makes sense to do similar iterations while shifting everywhere as well. In no time, a fully secured software product will be realized.\nA future article will introduce the concept of a security product, to expand upon the premise of Shift Everywhere.\n",
    "ref": "/blog/shift-everywhere/"
  },{
    "title": "Opensource AppSec for DevOps",
    "date": "",
    "description": "Adversaries appreciate your slashed AppSec tools budget, but you don't!  Here is a way to fill the gap with with opensource alternatives.",
    "body": "Introduction Q: Is it possible for a commercial enterprise to implement an application security program with only opensource tools?\nA: tl;dr – Yes, it’s possible!\nOftentimes organizations face a lack of funding to implement a commercial, “turnkey” application security solution. As an alternative, it might be worth evaluating opensource solutions. It\u0026rsquo;s important to note the term opensource comes with the understanding that many “free” application security tools are not “free” for commercial use. This post provides true opensource alternatives for companies looking to fill gaps within each phase of the development lifecycle that won’t impact the budget or cause a legal/licensing issue.\nSelecting security tools for modern software stacks, homogeneous environments, or small dev teams are relatively straight forward to select and implement. However, at the core of many organizations, there exists technical debt and legacy systems. Corporate enterprises also have the added burden of a diverse set of languages and frameworks that have either grown organically, by acquisition, or both. Another issue is that large teams or companies with many development teams need a way to effectively scale application security to all developers and to cover most all technologies. The article addresses the latter.\nLifecycle \u0026ndash; DevOps, DevSecOps, Secure Development Any term is appropriate for describing the development approach when it comes to application security. For simplicity (and brevity), it will be referred to as “SDL”. Regardless of methodology, iteration speed, or codebase, security should not be considered a blocker. Security can be injected within each phase of the SDL without adversely impacting development, while simultaneously improving the quality and security of the final product.\nThe following phases are typically present in most SDL’s. The sections below include opensource products that match capabilities commonly found within each phase.\nPlan Threat modeling is an activity to discuss what can go wrong. Most modeling can be accomplished with a meeting, whiteboard, and dry erase markers. If whiteboarding doesn’t cut it, both Microsoft and OWASP provide free tools. Microsoft\u0026rsquo;s is called “MS Threat Model” and OWASP\u0026rsquo;s is called Threat Dragon. Both have very good capabilities.\nBTW, OWASP will be mentioned a lot in this post due to their commitments to opensource and application security.\nCode Static Application Security Testing (SAST) is security-based code analysis. Linters, code style, and code quality tools exist for just about every language, but they are not focused on security. There are also security-based static code analysis for individual languages that may not have quality aspects to them. SonarQube is a popular tool because it provides a little bit of both quality and security wrapped into one tool that supports many languages. The basic version is free for enterprise use, and enterprise licensing is offered as well. SonarQube identifies what it calls “Security Hotspots” and “Security Vulnerabilities” in code for the more popular languages it supports. If a better SAST tool is eventually implemented by your enterprise, SonarQube is still worth keeping around for code quality and code style purposes.\nBuild CycloneDX is a Software Bill of Materials (SBOM) standard that identifies a format for listing the components that make up a piece of software. CycloneDX is another OWASP project and could also be included under the \u0026ldquo;Plan SDL\u0026rdquo; phase above because it helps to understand the components that make up your software inventory. It is placed in the build phase because it is generated by incorporating it into the software’s build process. Many plugins are offered to create an SBOM listing based on your language and build solution.\nSoftware Composition Analysis (SCA) looks at vulnerabilities found within opensource components. Dependency Track is another OWASP project that ingests an SBOM file and uses it to validate against several freely available threat intelligence and vulnerability databases. It incorporates results into a web application that quickly sets up and runs on Docker.\nA Binary Repository is a place to secure your own build artifacts, containers, and 3rd party components thru a local proxy. Like SonarQube, Nexus has a “Freemium” consumption model that includes plenty of language support and integrations.\nSecurity Automation? Yes, I could have used the term CI/CD. Yes, it makes sense to integrate security checks into continuous integration and deployment scenarios. However, it may not always be practical, culturally appropriate, or there are just too many roadblocks. Regardless of what it\u0026rsquo;s called or where it takes place, Security Automation encompasses any phase of the SDL where automation makes sense. There are many automation solutions in this space and Jenkins is one example, or simply a good scripting language like Python or PowerShell can be utilized to automate application security needs.\nTest Dynamic Application Security Testing (DAST) identifies flaws in web applications. Items like server misconfiguration, cross-site scripting, and business logic errors. Unlike SAST, the exploitability can be proven with penetration tests (pen tests). OWASP Zed Attack Proxy (ZAP) was developed for pen testing and it can be automated. Another pen test product, Burp Suite provides a freemium version but it’s ability to automate is limited with the community edition. Both products have a strong community behind them, so you don’t have to be an experienced hacker or start tests from scratch.\nRelease/Deploy Configuration Management was often exploited by adversaries in the early days of cloud and in the creation of cloud-native applications. It still is! The Shared Responsibility Model states that cloud providers are responsible for security of the cloud, and their customers are responsible for security in the cloud. It means that cloud providers will give you a securable environment, but it’s ultimately up to you to configure it securely to meet your needs.\nThe Quay project provides a Docker container vulnerability capability with a scanner called Clair. It is also the basic image scanner for AWS ECR.\nInfrastructure as Code (IaC) is a best practice for cloud and infrastructure. Automation here makes sense, especially in cloud environments where it is simple to both provision and tear down infrastructure. Just as it is easy for an engineer to misconfigure infrastructrure manually, it is easy to misconfigure infrastructure as code (IaC). Only with IaC, the security vulnerability can be propagated across an environment. KICS, by Checkmarx provides a scanning capability to help identify issues prior to deployment.\nScoutSuite provides a complete cloud account scan that identifies cloud security misconfigurations. It is easy to set up and with some security automation can be used to oversee a fleet of cloud accounts.\nOperate In Application Security, operations is about the ongoing maturity and assurances of the software product. An operational area where application security provides value is with Maturity Modeling. The determination and measurement of the security level of software is valuable from compliance, audit, and strategic perspectives. A great way to build a solid software assurance capability for the enterprise is to implement one of the following models that best meet its needs.\nOWASP Software Assurance Maturity Model (SAMM)\nOWASP DevSecOps Maturity Model (DSOMM)\nMinimum Viable Secure Product (MVSP)\nPros and Cons The primary benefit of opensource application security tools mentioned here is that they are ok to use in a commercial environment without a budget impact. All products mentioned have broad community support and some are backed by a for-profit enterprise offering freemium editions.\nA few drawbacks of an entirely free opensource application security stack is the lack of perceived quality, and the amount of ramp up time. Nearly all of the tools require infrastructure to run and ongoing maintenance. Also important is the resource skillset, and availability to both operate and maintain.\nSummary Determining Total Cost of Ownership (TCO) when evaluating opensource tools to commercial tools is difficult to measure. It comes down to the age-old question of “Buy versus Build”. The opensource route is akin to build given the increased time and effort needed to stand up all the tools, compared to buying a commercial off the shelf (COTS) application security solution.\nOne thing is for certain, any effort to improve the security posture of applications is better than no effort. Doing so might just upset those happy adversaries!\n",
    "ref": "/blog/opensource-security-tools-across-the-devops-lifecycle/"
  },{
    "title": "The Graph (GRT) Whitepaper Review",
    "date": "",
    "description": "Making Web3 Queryable",
    "body": "Premise Searching for meaningful information from blockchain data and decentralized storage (IPFS) is extremely difficult. To begin, data stored on a blockchain is transactional and based sequentially upon when the transaction was added to the chain. A traditional database is relational but blockchains are not. Imagine having to search thru every block beginning with the genesis block to the most recent to find what you are looking for. It will not only be slow, but the amount of processing and transformation into something meaningful is cumbersome and requires that you do it all yourself. Not to mention, the solution would be centralized and proprietary.\nThe Graph Value Proposition Originally defined as a Decentralized Query Protocol, The Graph provides the tools and the community network to create a simplified and fully decentralized means to query data that has been indexed into what is called a subgraph. Data in context is information, and the capability to query information is what makes The Graph so valuable for use in the Web3 dApp universe.\nTechnology Categorization Type: Web3 Query and Index. A means to provide dApps with decentralized-sourced data (blockchain and storage) upon which to easily use. A decentralized network for providing data in context, i.e. providing Information in a queryable way. The specific information is termed a subgraph. Project Differentiator: Current approaches require dApp developers to create their own solutions. The Graph provides the tools and the network community to simplify inclusion of blockchain information into dApps. Original Whitepaper and current documentation. Consensus: Proof of Indexing. I think of this as the quality of information the subgraph provides. The better it is, the greater the value. If it is deemed untrustworthy it risks getting slashed. Token: GRT. It is used for all aspects within The Graph network. Paying for a query, as a staking coin, to compensation for all the entities which are needed to make the network community to operate. The Network Entities There are several entities that make up The Graph Network. The Graph calls them Network Roles. Images taken from TheGraph.com.\nEnd User The Client is the dApp running on the end user\u0026rsquo;s device that queries a particular subgraph. The end user receives the information which is provided thru the particular dApp calling it.\nDeveloper It is a developer who creates a subgraph. The developer indexes based on a smart contract on a Graph-supported blockchain. The subgraph is then deployed to a Graph Node.\nAlso, it is the developer who builds a dApp to query a subgraph thru a GraphQL endpoint. Accomplished in much the same way a developer will program a call to an API, using GraphQL\nIndexer Indexers are Node Operators. They provide indexing and processing services. The Indexer is required to stake GRT to operate a node. At time of this writing, the minimum amount is 100,000GRT. Indexers can also have GRT delegated to them from the Delegators. Think of this as Delegated Proof of Stake (DPOS), See Delegators below.\nIndexers earn query fees and index rewards. The Indexer selects which subgraphs to index, in part based on what curators think about a particular subgraph.\nIndexers use the Curators recommendations (signal) to process subgraphs as a determinant of which ones to Index.\nCurator Curators signal subgraphs that they feel should be indexed. Think of this as the Influencer group, with \u0026ldquo;skin in the game\u0026rdquo;. Curators will look at The Graph Explorer and determine if the subgraph has enough value to it to stake GRT on it. The curator stakes GRT on specific subgraphs to earn royalties. A developer is likely to curate his own subgraph.\nBecoming a curator requires depositing GRT following a bonding curve to mint shares of the subgraph. The amount of minted shares is the proportional size of your ownership. Since signaling uses a bonding curve to determine your subgraph shares, the sooner you decide to Curate a subgraph, the greater the potential reward. A developer will be first in, because they wrote it and believe it contains great potential. If you think it\u0026rsquo;s a great subgraph too, you will want to signal as early as possible.\nThere are risks associated with Curators. Risks such as a bad or buggy subgraph. A subgraph that is not used will not earn query fees, so no rewards for you. You are subject to a 2.5% curation tax that you will not get back.\nIn essence, you are depositing GRT (staking), to mint shares of the subgraph (ownership shares in return). The shares proportionately allocate the royalties that are earned for the subgraph. Getting in early to a particular subgraph means you\u0026rsquo;ll need to stake less GRT than someone who gets in later. To unsignal, you will need to burn your subgraph shares to get your GRT back after a waiting period.\nDelegator A Delegator stakes GRT to Indexers. As stated above, this is similar to DPOS when it comes to staking governance tokens to allow others to vote on your behalf. The benefit of doing this is that you earn a portion of the Indexer rewards and the Query Fees. So, if you own GRT and want to have an opportunity to earn while hodling, you can become a Delegator.\nUnderstand everything before you do it!\nThere is a charge (.5%) to delegate. There is an unbonding period of 28 days before you get your GRT back, and you stop earning query fees. Choose your Indexer wisely! The indexer decides your cut. They can\u0026rsquo;t steal your GRT, but they can take advantage of you in some ways. All this info is found in The Graph Explorer. The Delegator role is the path of least resistance and the least technical to become a player in The Graph Network. Do you want to know more about the Delegator role? Here is a great introductory video for prospective Delegators.\nGraphQL Security Considerations It is no coincidence The Graph\u0026rsquo;s name comes from the Graph Query Language (GraphQL). GraphQL is a carryover technology from the Web2 world and was developed by Facebook as an alternative to JSON for querying API\u0026rsquo;s. Secure development considerations also apply in Web3. An indexer isn\u0026rsquo;t going to list your subgraph if they think it is a security risk. OWASP API Security is your friend when it comes to subgraph development.\nValidate Input Add pagination to limit data being returned. It reduces unexpected usage costs, keeps queries performant, and keeps indexers happy. Enforce rate limiting based on IP Implement timeouts to limit resource usage. It will also keep your indexers happy and will help reduce complaints within The Graph network keeping everyone else happy. Evaluate your code for nested queries. Nested queries are used in Denial of Service attacks. Remember to disable excessive error messages in production environments to limit risky information disclosure. Concluding Remarks Web3 developers face many challenges when architecting and developing data-rich dApps. Retrieving meaningful information from across the decentralized web is a huge challenge. The Graph provides a sustainable network ecosystem to query blockchain data by means of a simple GraphQL query from within a dApp.\n",
    "ref": "/blog/whitepaper-grt/"
  },{
    "title": "Is Your NFT Collectable Secure and Legitimate?",
    "date": "",
    "description": "The Treat Landscape for an NFT is a Minefield.",
    "body": "What is an NFT? NFT stands for NonFungible Token. Think of it as a digital record which has been minted (created) on a blockchain. Meaning, the file represents \u0026ldquo;something\u0026rdquo; that has been recorded on a blockchain. It is very unlikely the NFT itself is stored on a blockchain, rather the transaction details and a reference to the actual NFT location are stored on the blockchain.\nThere are many potential \u0026ldquo;gotcha\u0026rsquo;s\u0026rdquo; associated with NFT\u0026rsquo;s. This post looks at the NFT landscape. It is intended to help educate the NFT minter, the collector who purchases directly from the minter, and potential buyers later. Before you decide to swim with the sharks consider the following questions in 3 sections\nIs It Legitimate? Can My NFT Disappear? Ownership Is It Legitimate? This section covers a few of the ways an NFT might not be the original, may not be legitimate, or fail to have proof that it is the real thing.\nHow do I know? In short, you do not. At least not without additional proof, that is. As with real non-digital collectables, the collectable and its provenance could be faked. The blockchain alone, merely provides an immutable record of the transaction(s), not the legitimacy of the NFT being purchased. If the NFT has been bought and resold multiple times, it should still be traceable back to the original minter, and you can independently verify the individual/company is the actual entity that originally created it.\nBuying thru reputable exchanges, or from a known and verified buyer, limits the risk. Additionally, reputable exchanges should also have seller rating capabilities. But even like eBay, and other reputation-based systems, there are ways to deceive and mislead.\nCan someone replicate my NFT? In short, yes, they could. In the real world, there are all kinds of faked products, art, and collectables. In the virtual world of NFT\u0026rsquo;s the same can happen. The blockchain transaction cannot prove the NFT is real, it only records an unchangeable record of the transaction. It is not intended to know if the NFT is real or fake.\nCan there be multiple copies of the same NFT? In short, yes there could be. Minting is the act of creating an NFT transaction recorded on a blockchain. With so many exchanges selling NFT\u0026rsquo;s these days, nothing stops an individual from minting the same NFT across multiple blockchains.\nThe risk could be mitigated by knowing in advance which blockchain or Dapp the Minter always uses. However, any seller or a criminal could mint a slightly modified NFT on the same or any blockchain. A common storage location for NFT\u0026rsquo;s is on IPFS. which uses a unique hashing algorithm (more below). A slightly modified NFT would result in a different hash, making one believe it is original and authentic, when it is only a copy. Now which one is the original?\nCan someone else mint the same NFT? In short, yes someone could. Since a blockchain transaction is a public record of the transaction. A thief has all the information about the transaction available to them to set up a fake or misleading identity and list the NFT for sale again.\nCan my NFT disappear? In short, yes it could. It will only stick around for as long as it is stored and supported. This section discusses some of the potential issues around NFT storage.\nBlockchain technology and Web3 in general is in its infancy. If the developers behind the particular blockchain that recorded your NFT decide to stop supporting the project, the entities processing transactions could potentially cease as well, meaning the database could go away, leaving no record of ownership.\nAnother scenario would be if the storage location of the NFT was deleted, or in some other manner ceased to exist. IPFS (InterPlanetary File System) aims to make the file stick around forever in a Web3 sort of way. While IPFS is also in its infancy, the file (or any file for that matter) is likely to be hosted and cached by a single or related set of nodes that are managed by the Marketplace selling the NFT. If the Marketplace goes by-bye, so might the storage, and so will your NFT.\nWhile it is not an absolute that it is gone forever, the link from the defunct blockchain will be missing. It may be possible to execute another block transaction or mint new, to update the reference to the NFT, to keep the NFT on-chain (different or same chain). But then again, someone else could do that as well before you do.\nMy NFT is stored on IPFS, isn\u0026rsquo;t that how it should be stored? As alluded to in other sections, IPFS (InterPLanetary File System) is focused on content, not a centralized domain, and is a great way to store NFT\u0026rsquo;s. This means the NFT is assigned a unique identifier called a ContentID. This ID is unique to the NFT. If the NFT changes in one way or another, a different hash is generated. The contentID (a unique hash) will never be the same unless it is the same NFT. When you purchase your NFT, you may be purchasing the ContentID. At least that may be what is recorded on chain. Aside from the file going away, as discussed previously, a cloned file with a single pixel changed would generate a different ContentId, even though there is no way to tell which one is the original and which one is the fake. Other than comparing the ID with the original on-chain you will not know. The storage node could go down, or malfunction in some way and without active backup nodes, bye-bye NFT.\nThe blockchain is where the reference (pointer) to the NFT is stored. If one or both go away, the NFT you have becomes difficult to prove what you have in your possession is the original.\nWhat if the marketplace where I purchased my NFT goes out of business? Not a guaranteed loss of your NFT. It will still be necessary to maintain the chain of ownership and that the reference to the NFT is still valid. It is important to understand exactly how the marketplace conducts business and understand how they manage the lifecycle of an NFT to adequately evaluate the associated risk.\nOwnership Congrats on your NFT purchase! It is now a collectable in your crypto wallet. The same wallet you guard with your private key and/or private phrase. As previous sections alluded to potential risks, this section attempts to show you what you have.\nA strength of blockchain technology is that it is an immutable record of a transaction. What is stored is the proof that you own the NFT. The block will contain basic information about the transaction. For example, the NFT cost, gas cost, wallet addresses involved (yours will be one of them), smart contract information, information about the NFT, date, time, etc.\nWhat did I buy, exactly? Interestingly, what is owned seems to vary by NFT Market. Here are 2 examples\nWhen you purchase a CryptoKittie, you are limited with what you can do with it. As described in this CryptoKitties Blog post.\nLet us follow a simple NFT transaction at another marketplace, named SuperRare. Following the transaction details on the block explorer. We can see that an NFT was minted #24478, and is for sale. Digging into the token, we can query the contract to find a string with the TokenID, representing what was for sale: https://ipfs.pixura.io/ipfs/QmW2HMPEkkudU9zEzGDJ8ZEhJLx21UDbXTWT1XNCGUEDmE/metadata.json. The file points to where the actual NFT is stored. The NFT is NOT stored on the blockchain, the link above is what is stored. Following the link returns a description of the NFT. Digging into it you can find where the NFT is actually stored. If this site ever goes away, or has something catastrophic happen to it, the NTF associated with the transaction is gone. If you have the file saved on your computer as a backup, that is not what the transaction listed on-chain shows. You have lost the ability to prove that this is a paid-for collectable, because what is on the chain can\u0026rsquo;t be proven any more.\nRecall, the block transaction references the a file with a reference to the IPFS ContentID of the directory where the NFT resides, as it appears thru an IPFS gateway. If that gateway goes away, the blockchain reference will not accurately point to the NFT. However, utilizing the Brave browser, if it remains stored somewhere on IPFS, it can still be retrieved ipfs://bafybeihq2k6oscadxj6mfax6h3ofcjqeu2iseeonget5m6huliaeb4ypza/, but this is not how it is recorded on-chain. The assumption remanis that it is stored somewhere on IPFS. Who is going to insure that will happen?\nA future article will delve into the details of an NFT transaction to help readers better understand blockchain transactions and how to identify risk.\nNFT disputed ownership? There is no dispute resolution, arbitration, or global NFT court that exists for NFT\u0026rsquo;s should a dispute arise. However, it might be good to understand a marketplace\u0026rsquo;s dispute resolution policy. Also, how crypto is traded, whether there is an escrow, a hold period, or refund process.\nMy NFT shows up in my wallet, that means I own it, right? Yes, it means you own something. As previously written, it may not actually end up being what you think, even though it looks like it.\nIn Summary The intended purpose of the FAQ is not to give the criminal minded ideas on how to commit fraudlent activity. Rather, it is intended to help the individual to better understand the risks associated with NFT\u0026rsquo;s. As was documented here, having a transaction stored on some public blockchain in and of itself is not the end-all, be-all to NFT ownership.\nI personally have not found cases of widespread fraud around NFT\u0026rsquo;s as of this writing. As Web3 and NFT\u0026rsquo;s gain in popularity, along with a Fear of Missing Out (FOMO), unsuspecting collectors may unwittingly enter into transactions without fully understanding what they are getting into, and that is what the fraudsters are hoping for.\nBuyer Beware!\n",
    "ref": "/blog/nft-security/"
  },{
    "title": "Starting an Application Security Program",
    "date": "",
    "description": "It's easy to purchase tools, but getting ROI and reduced risk is something that require a little hard work.",
    "body": "Originally, I authored this article for the Veracode Community Website.\nI’ve Got the AppSec Tools, Now What? An article for the Veracode Community:\nFantastic! Your org has purchased shiny new products that will seriously up your game against those pesky adversaries and take a bite out of risk. Often, the first question asked after such a purchase is, “now what?” The purpose of this article is to provide some thought provoking tips aimed to keep AppSec moving along.\nUnderstand Your Landscape Every industry, every company, and every team is different. Consider number of dev teams and total number of developers. Each company functions a little different from the next. Companies are organized, have diverse company culture and varying levels of software maturity. Wow! A lot of factors play into determining next steps, and ultimately the success of Application Security within your company. So, where do you start? Let’s consider a few more things.\nSize Matters Do you need to be a program manager or a hands-on devSecOps engineer? It depends upon the size of the organization, the number of software products, teams, and developers. Are you going to need to be hands-on-technical running the tools, automating scans and mitigating flaws? Or do you need to be a super-energetic program manager specializing in cat herding? Maybe something in between? Is AppSec going to be a team of one or a few? Will there be roles to fill? The aligned resource(s) should be reflective size. For example, a heads-down programmer who knows Application Security inside and out might lack skills necessary to lead a multi-national, multi company enterprise of 100’s or 1000’s of developers. However, in other scenarios that individual makes all the sense in the world.\nKnow Your Dev Teams What does your appSec inventory look like? If you don’t have one already, it’s a good first step to get one started and commit to maintaining it. What programming languages are being used? How about development tools? Are teams Agile, Waterfall, or Wild West? Even for a company with one agile “2 pizza team”, an inventory will help in the formation of an appSec roadmap.\nIf you can’t embed within a team, the more you interact with them and speak their language, the greater chance for success. You don’t want to be the security personality that developers run away from. Rather, cultivating open and honest communication will help in the long term as you share in the success of high quality secure releases that lower application risk for the organization. As a check point, are developers coming to you asking questions, and is your only communication to developers, “Just fix your darn code”. As a developer who would you rather work with?\nIn large orgs, finding a Security Champion to help carry the AppSec banner will help multiply the efforts. As your program succeeds and grows, your time may start to spread thin. Champions can help with setting up scans, automation and mitigations, just to name a few activities. These individuals will have or develop the same AppSec passion that led you to where you are today. Security Champions will be a great resource going forward. A champion per team is a good goal to have in large environments. The individual will help keep things moving when you can’t be there yourself. Ultimately, all developers need to become secure developers, and Champions are a first step to help get there.\nLeverage Your Veracode Resources Your assigned account manager, customer success manager, and solution architect. They usually come with the product and their Raison d’Être is to make you successful! It’s important to get to know them, and keep the lines of communication open and prioritized. Challenge and hold the Veracode team accountable to help you leverage and get as much value from the tools as possible. Meet often, set goals and work together. Leverage consultation calls to get help get to the root of the problem with the hard issues. Consultation calls are a great way to get down to the core of the flaw quickly.\nThe Veracode squad is just as important as the tool itself. Lastly, leverage your two new favorite websites: Veracode Community Website and Veracode Help. User Access to the Veracode Platform Implement single sign on to Veracode if your company has the capability. Make it easy to get on Veracode to look at results. Regarding team size, think about the various roles, groups, and views that might be needed for accessing the platform. It is tempting to go with a flat structure where everyone sees everything, but you might be limiting yourself or creating extra work down the road with one all-encompassing team. A Baby Bear (just right) approach to teams, roles and groups will help not only for access, but also for reporting metrics and showing the value of Application Security.\nDepending on how you roll out the platform (you do have a plan for that, right?) The number of users will likely grow. As the user base grows, it might be helpful to have a slide deck or develop a consistent step-by-step Veracode website walk thru to show new users. Be prepared to demo the Veracode website over, and over, and over again. Given Veracode’s automation capabilities, users may not need to get in very often, so it’s good to have reminder sessions to help keep the platform fresh in their minds.\nMaturity It’s hard to go from 0 to 60 miles per hour in 1.99 seconds. Unless you are in a Tesla, it takes time to get up to speed and mature. A roadmap is a good way to show how the program will mature. Those executive-types love to see what you plan to do, how it will help reduce risk, and when you intend to get there. Will it be iterative? How many iterations? What percentage of repos will you be scanning and by when? How will tools be rolled out? Will you take a team-by-team approach, or roll out one tool at a time? At what point will automation be implemented? Follow the roadmap and adjust as needed. Each iteration is a notch in your maturity belt and a step closer to Plaid Speed!\nShow Your Value Your company has invested in AppSec tools, and most importantly, they’ve invested in YOU! If they didn’t believe you were the best person for the job, you wouldn’t be doing it. Success depends on your ability to show that the program is making things better. It is by showing how you are leveraging the tools, how teams are performing, and ultimately how you are performing. Using Veracode’s reporting capabilities is a great way to show this value.\nVeracode comes with great out of the box reports that can be customized by role. It is possible to make a dashboard that shows exactly what a user needs to see based on their permissions, by leveraging the access groups you determined in the section above. A dashboard for each team that aggregates up the food chain provides a lot of value. For example, the team dashboard aggregates to a director’s dashboard that aggregates to a CISO/CIO view. An example of a metric might be a simple aggregated percent compliant metric for a CISO representing the whole enterprise. Whereas, a listing of code bases that pass or fail compliance would be more meaningful to a busy director. Of course, development teams need to see the nitty gritty details as feedback to identify and mitigate.\nIn Summary The article wasn’t about the technical aspects of the products and services you purchased. It was intended to help aspiring AppSec program managers and engineers to get a glimpse into the softer side of starting an AppSec journey.\n",
    "ref": "/blog/starting-application-security-program/"
  },{
    "title": "EOSIO Blockchain Enables Secure and Permissioned Smart Contracts",
    "date": "",
    "description": "A Blockchain for building dApps",
    "body": "Premise The EOSIO blockchain is utilized by several decentralized projects. Understanding the permissioning capabilities of the platform will help to gain an understanding of not only the EOSIO platform, but also how permissioned blockchains differ from fully permissionless blockchains.\nEOSIO Value Proposition EOSIO is a platform for building and vertically scaling decentralized applications (dApps). EOSIO creates an operating-system-like platform from which dApps can be built and maintained. EOSIO considers authorization, RAM storage, database, and asynchronous communication clusters scaling to millions of transactions per second.\nIn contrast to Ethereum-based smart contracts, EOSIO-based dApps do not necessarily need to charge the user fees for interacting with the platform. Rather, costs for interacting with the platform are paid for by either the dApp creator or the account owner. Not all use-cases require a public user to pay for a particular service. Consider consortium, enterprise, internal, or private decentralized systems use cases. Real Estate, Insurance and Healthcare come to mind as examples where one would want data to be protected and not publicly accessible. Other use cases could follow a web2 (or web2.5) strategy where dApps could implement legacy monetization strategies. EOSIO attempts to be applicable to any number of use cases.\nTechnology Categorization Type: level one, permissioned (restricted) Blockchain Project Differentiator: Built with developers in mind. It aims to be a decentralized computing operating system while applying a security and access model to enable more use cases than public-only blockchains. Whitepaper Consensus: Delegated Proof of Stake (DPOS) Token: EOS. Each project which forks the EOSIO codebase will have its own crypto currency. By looking at the blockchain explorer, blocks.io, we can see a list of projects each with their own EOS based token. Security Considerations Accounts Accounts identify a participant on the EOSIO blockchain. A participant can be an individual or a group. They also represent the smart contract actors that push and receive actions to and from other accounts in the blockchain. Actions are contained within transactions whose actors are accounts.\nPermissions Permissions control what accounts can do, and how actions are authorized. Account permissions are used to authorize actions and transactions to other accounts. Permissions can be structured so that a particular named permission’s authorization can be satisfied with implicit authorization (if the parent is authorized, then the child is implicitly allowed).\nThe permissions table includes all permissions and any associated hierarchy dependencies within those permissions. Each permission is linked to an authority table. The table utilizes “thresholds” before an action associated with a permission can execute. The authority table associated with the permission contains not only the account’s threshold weight, but also any accounts which may execute on behalf of the account permission. The weighting structure grants more trusted users a greater weight, or requires accounts with lesser authority to collectively participate (add together) to meet a certain weight. The weighted solution may result in privilege escalation by a lower weighed user increasing weight to gain greater privileges. Understanding how the platform minimizes this threat is outside the article’s scope.\nAll accounts will have Owner permission and Active permission. All permissions utilize public key and private key pairs for each permission. Private keys associated with each permission need to be protected. The Owner permission private key should be kept in cold storage since it should only be used for account recovery. Like a Root account, it should not be used to administer or operate the account. The Active permission would be used to make account changes. Additional permissions can be created (along with key pairs) to do whatever may be necessary. From a security perspective, this is where least privilege must come into play. For any of these permissions, the private key is utilized to sign the transaction in order to validate the public key.\nIt is this mechanism which determines how Actions and Handlers behave when they are received. It is this interaction between (and among) accounts with the defined permissions associated with the Actions and Handlers which define a smart contract. The permissioned architecture is what allows transactions to be secured and how Confidentiality and Integrity is upheld.\nConcluding Remarks There is a debate in the web3 community regarding permissioned versus permissionless blockchains, and to what degree of decentralization one may have over another. Both solutions seem to have strengths and weaknesses. I have not found a one size fits all, silver bullet. I do not attempt to judge one approach over another. Rather, I look at the architecture, the approach to securing the platform, the smart contracts, and the data of each decentralized solution.\n",
    "ref": "/blog/whitepaper-eos/"
  },{
    "title": "What is a Dlog?",
    "date": "",
    "description": "The article explains a Dlog",
    "body": "The article answers the question of \u0026ldquo;what exactly is a \u0026ldquo;Dlog\u0026rdquo;? No this is not a typo. Blogs are so Web2. A Dlog, is a concatenation of Decentralized and Blog. Wow! Real original. What is interesting is that at the time of this writing Googling Dlog did not return anything remotely close to a distributed or decentralized web log. As far as I know, this is the first documented use of Dlog in the web3 context. While I know that I am not the first to put a blog site on the distributed/decentralized web, I believe that I may be the first to refer to a distributed/decentralized blog as a Dlog.\nUnless you are stuck in pre 2000\u0026rsquo;s web 1.0 world, then you are definitely aware of what a blog is, so I won\u0026rsquo;t bore you with that. What I will bore you with is the difference between your run of the mill blog and a dlog. First and foremost a dlog is the same as a blog, a collection of personal musings of an individual, only a Dlog is stored and distributed in a decentralized manner. You are receiving this this content from one or many IPFS nodes. IPFS (InterPlanetary File System) does not operate on centralized or top-level domains (.com, .org, etc), rather they operate on the content itself. The content can be stored anywhere, on any IPFS node. Much like Peer-to-Peer BitTorrent, or even the Onion protocol, taking down one node will not remove the content. It makes the content incredibly resilient and censor resistant from both nations and tech companies wishing to silence those with which they disagree.\nYou most likely got here by a reference to DeCaPa.io, which is my legacy DNS name. However you could have gotten here just as easily utilizing any one of the following:\nMy Dlog Site Content Identifier (Brave): ipns://k51qzi5uqu5dix750h2z9pwevsj3bhc74djcu80plav5agh6mpr40z11bx8wtu/ My ENS name (Brave): ipns://matteo.eth Using an ENS / DNS bridge (any browser): https://matteo.eth.link or https://matteo.eth.limo At the time of writing only the Brave Browser natively supported IPFS/IPNS. Chrome required the IPFS Companion plugin. Hang tight, I will write more about the awesomeness of IPFS in future articles.\nRest assured, this dlog is built on Web3, it utilizes IPFS to store the content you are reading, and it is all stored by node peers that can be running pretty much anywhere. I became so fascinated with Web3 that I decided what better way to learn more about it than to start playing with it. As I began learning, I started putting my thoughts down and before I knew it, I was putting a blog together to understand IPFS. With this technology, I can post content on the decentralized web and not require a server or a domain from which to serve the content. It is the content that is important, and why Content is King!\nWhy is this significant? It is because the web is becomming increasingly centralized. When you think about it, the internet is controlled by a small number of like-minded, and immensely powerful group of companies. The companies give their services away in exchange for your data, and your precious time \u0026amp; attention, from which they make Trillions of dollars. Your personal privacy, browser settings, even your clicks are fully understood and exploited using big data algorithms, so that that they know you better than you know yourself.\n",
    "ref": "/blog/decentralized-web-blog-dlog/"
  },{
    "title": "About",
    "date": "",
    "description": "About the creator of this Decentralized Blog, or Dlog, if you will",
    "body": "Podcasts I listen to often ask their guests to provide an “origin” story about their background and experiences that led up to their current field of work or interest. Here is mine …\nI became fascinated with the internet in the mid 90’s when I realized it would transform the world. After purchasing my first “real” computer, I began learning as much as I could. BTW, “first real” computer to me a is a computing device with a hard drive, and modem. I’m not counting the 80’s era TI-99-4a from my youth. Don\u0026rsquo;t you love the cassette tape for mass storage?\nI also don\u0026rsquo;t consider the DOS 1.0 Compaq luggable that I traded my guitar and amp for in college either. I\u0026rsquo;m thinking I got the worst of that deal, because the guitar and amp would still be working, and I\u0026rsquo;d surely be a rock star by now. Everything that was necessary to learn and do to create a website led me to eventually leave my job at a bank and jump into a passion that was love at first sight. I added a section to my site called “the proposal”, which documented how I asked girl friend to marry me. Even though the term hadn’t been named yet, I created a blog! The sites primary draw was “An Introduction to the Internet”, and it opened a few doors for me. I still have an email from alan@yahoo.com, who let me know that he had categorized my website for Yahoo. Still funny when I think about how small Yahoo must have been to give its employees an email with only a first name.\nAfter taking a few classes at a local university, I obtained an IT position. I attended university part-time eventually earning a degree in Computer Programming Technology. It led me to a programmer analyst position which allowed me to gain experience and skills. The degree opened a few doors for me, but it was my passion and love of the tech which enabled all the opportunities that I’ve taken advantage of while rising thru the ranks of IT.\nA few years passed, and again, I pivoted my career, taking knowledge and experience with me from application development, enterprise architecture, and integration architecture to wrap that into a focus on security and protecting data and applications. Application Security became my passion, and I took great satisfaction with identifying and helping developers and product teams ensure a secure product from the start of development. I felt that getting security correct from the beginning to prevent an offensive attack against software in production was the best approach. Security-first, entailed awareness, training, and the right tools at the right time to create a secure product.\nAlong with creating secure software in the new world of cloud computing, it is additionally necessary to ensure the product is configured securely as well. A misconfigured application is as big of a problem as is producing insecure code. In the world of Infrastructure As Code, there really isn’t a difference, is there?\nMy career has focused on protecting an individual’s right to have a secure virtual presence, while working for various companies. As big tech business models monetized the individual’s personal information, they became the wealthiest companies the world has seen and maybe more powerful than individual nations themselves. The rights of individuals and an individual\u0026rsquo;s right to privacy doesn’t exist anymore. The tech oligarchs provide free services and tell us we should be happy with it and accept the control they have over our lives. Remember, if you don’t have to pay for it, you are the product. The individual has become an indentured servant to these companies.\nBelieving there must be a better way. My search has led me to what many are calling Web3. It is where my passions have led and is the purpose of this Dlog. Thank you for coming along with me on my journey to what is certain to becoming the next “big thing”.\n",
    "ref": "/about/"
  },{
    "title": "My First IPFS-Based Post",
    "date": "",
    "description": "Attempting to understand IPFS by creating a Hello World Program",
    "body": "\rThrough the Looking Dapps\rFiguring out IPFS and Web3\rHello IPFS! I just created a very simple page using IPFS. Like all well-intentioned, beginner bloggers ... more to come!\rNavigation can be tricky in IPFS. You must have an understanding of Merkel Trees because changing will result in a changed CID. Also, it is important to make a blog that will run on both the IPFS protocol and legacy https. Another working title is \"My Dlog\", a combination of decentralized and log. It takes it's name from \"Blog\", which stands for Web Log. The ContentID will change every time the file is changed, but by using IPNS The name (reference to the content) will remain the same.\rWhile simple, the few pages, images, and links provide everything that I'd need to test to prove out a basic understanding of what is needed to correctly construct a static Dlog site hosted on IPFS.\rIn the future, I will utilze a blogging solution with pretty themes and get away from this pre-2000's web site look and feel.\r",
    "ref": "/blog/first/"
  },{
    "title": "Contact",
    "date": "",
    "description": "",
    "body": "The world of Web3 is diverse, changing, and fascinating! Tell me what you think about it.\n",
    "ref": "/contact/"
  }]
